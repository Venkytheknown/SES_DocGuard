vocab_size: 32000
merge_operations: 50000
min_frequency: 2
special_tokens:
  - <pad>
  - <unk>
  - <bos>
  - <eos>
  - <sep>
  - <cls>
  - <mask>
unicode_normalization: NFKC
preserve_case: false
max_sequence_length: 2048
learning_rate: 5e-5
batch_size: 32
num_epochs: 3
weight_decay: 0.01
gradient_accumulation_steps: 1
logging_steps: 100
evaluation_steps: 500
save_steps: 1000
seed: 42
fp16: true
output_dir: ./fine_tune_ses_docguard/models/finetuned
checkpoint_dir: ./fine_tune_ses_docguard/models/base
train_data_path: ./data/processed/*.json
eval_data_path: ./data/processed/*.json
evaluation_metric: F1
early_stopping_patience: 3
max_grad_norm: 1.0